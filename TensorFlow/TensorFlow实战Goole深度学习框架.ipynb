{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Courier New\">\n",
    "\n",
    "# TensorFlow 实战 Goole 深度学习框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一 Tensorflow 入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 TensorFlow 计算模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 TensorFlow 计算模型 - 计算图 Graph<br>\n",
    "<font face=\"Courier New\">\n",
    "\n",
    "Tensorflow 计算模型 -- 计算图: Tensorflow 所有的计算都会被转化为计算图上的节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "g1 = tf.Graph()\n",
    "with g1.as_default():\n",
    "    # 在计算图g1中定义变量\"v\", 并设置初始值为0\n",
    "    v = tf.get_variable(\"v\", initializer=tf.zeros(shape=[1]))\n",
    "\n",
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    # 在计算图g1中定义变量\"v\", 并设置初始值为1\n",
    "    v = tf.get_variable(\"v\", initializer=tf.ones(shape=[1]))\n",
    "\n",
    "with tf.Session(graph=g1) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    with tf.variable_scope(\"\", reuse=True):\n",
    "        print(sess.run(tf.get_variable(\"v\")))\n",
    "\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    with tf.variable_scope(\"\", reuse=True):\n",
    "        print(sess.run(tf.get_variable(\"v\")))\n",
    "\n",
    "# g = tf.Graph()\n",
    "# 指定计算运行的设备(本设备安装的并非gpu版本, 不可用  )\n",
    "# with g.device('/gpu:0'):\n",
    "#    a = tf.constant([1.0, 2.0], name=\"a\")\n",
    "#    b = tf.constant([2.0, 3.0], name=\"b\")\n",
    "#    result = a + b\n",
    "#    with tf.Session(graph=g) as sess:\n",
    "#        print(sess.run(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Tensorflow 数据模型 -- 张量 Tensor (TensorfFlow 的核心)<br>\n",
    "<font face=\"Courier New\">\n",
    "\n",
    "张量是 Tensorflow 管理数据的形式, 在 Tensorflow 中所有的数据都通过张量的形式来时组织的.<br><br>\n",
    "其中零阶张量表示标量(scalar), 也就是一个数; 第一阶张量为向量(vector), 也就是一个数组; 第n阶张量可以理解为一个n维数组. 但张量在 Tensorflow 中的实现并不是直接采用数组的形式, 它只是对 Tensorflow 中运算结果的引用. 在张量中并没有真正保存数字, 它保存的是如何得到这些数字的计算过程."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.constant 是一个计算, 这个计算的输出结果就是一个张量\n",
    "# 此处将张量保存在变量 a 中, 所以变量是一种特殊的张量\n",
    "a = tf.constant([1.0, 2.0], name=\"a\")\n",
    "b = tf.constant([2.0, 3.0], name=\"b\")\n",
    "result = tf.add(a, b, name=\"add\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Courier New\">\n",
    "\n",
    "从结果看出, 一个张量主要保存了三个属性:  \n",
    "1.名字(name)  其中\"add\"为节点的名称, \"0\" 表示当前张量来自节点的第几个输出  \n",
    "2.维度(shape) 该属性描述了一个张量的维度信息  \n",
    "3.类型(type)  每一个张量都有唯一一个类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Tensorflow 运行模型 -- 会话 session\n",
    "<font face=\"Courier New\">\n",
    "\n",
    "Tensorflow 中的会话(Session)用来执行定义好的运算  \n",
    "当默认的回话被指定之后可以通过 `tf.Tensor.eval` 函数来计算一个张量的取值, 以下代码展示了通过设定默认回话计算张量的取值."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 5.]\n",
      "[3. 5.]\n",
      "[3. 5.]\n",
      "[3. 5.]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1.0, 2.0], name=\"a\")\n",
    "b = tf.constant([2.0, 3.0], name=\"b\")\n",
    "result = tf.add(a, b, name=\"add\")\n",
    "\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    print(result.eval())\n",
    "\n",
    "# 以下代码也实现相同的功能:\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(result))\n",
    "    print(result.eval(session=sess))\n",
    "\n",
    "# tf.InteractiveSession 这个函数会自动将生成的会话注册为默认会话, 下面使用法:\n",
    "sess = tf.InteractiveSession()\n",
    "print(result.eval())\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 神经网络参数与 TensorFlow 变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 TensorFlow 随机数生成函数 (表)\n",
    "<font face=\"Courier New\">\n",
    "\n",
    "函数名称         |    随机数分布     |      主要参数\n",
    "---------------|---------------|:---------------\n",
    "`tf.random_normal`|      正态分布    |shape: 输出张量的形状，必选.<br>mean: 正态分布的均值，默认为0<br>stddev: 正态分布的标准差，默认为1.0<br>dtype: 输出的类型，默认为tf.float32<br>seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样  \n",
    "`tf.truncated_normal`| 正太分布, 但如果随机出来的值偏离平均值超过2个标准差<br>那么这个数将会被重新随机生成|shape: 输出张量的形状，必选.<br>mean: 正态分布的均值，默认为0<br>stddev: 正态分布的标准差，默认为1.0<br>dtype: 输出的类型，默认为tf.float32\n",
    "`tf.random_uniform`|平均分布|最小 minval, 最大取值 maxval, 取值类型 dtype\n",
    "`tf.random_gamma`|Gamma 分布|形状参数 alpha, 尺度参数 beta, 取值类型 dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# 声明一个 2x3 的矩阵变量, 矩阵的元素均是 0, 标准差 stddev=2 的随机数. 还可以使用 mean 来指定平均值!\n",
    "weight = tf.Variable(tf.random_normal([2, 3], stddev=2))\n",
    "with tf.Session() as sess:\n",
    "    print(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 TensorFlow 常数生成函数 (表)\n",
    "<font face=\"Courier New\">\n",
    "\n",
    "函数名称         |    功能     |      样例\n",
    "---------------|---------------|:---------------\n",
    "`tf.zeros`|产生全0的数组|`tf.zeros([2,3],int32)->[[0,0,0],[0,0,0]]`\n",
    "`tf.ones`|产生全1的数组|`tf.ones([2,3],int32)->[[1,1,1],[1,1,1]]`\n",
    "`tf.fill`|产生一个全部为给定数字的数组|`tf.fill([2,3],9)->[[9,9,9],[9,9,9]]`\n",
    "`tf.constant`|产生一个给定值的常量|`tf.constant([1,2,3])->[1,2,3]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros:0\", shape=(2, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "zero = tf.zeros(shape=(2, 3), dtype=tf.int32)\n",
    "print(zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 TensorFlow 也支持通过其他变量的初始值来初始化新的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2 的初始值被设置成了与 weights 变量相同\n",
    "# w3 的初始值则是 weights 初始值的两倍\n",
    "# 在 TensorFLow 中, 一个变量的值在被使用之前, 这个变量的初始化过程需要被明确的调用\n",
    "weights = tf.Variable(tf.random_normal([2, 3], stddev=2))\n",
    "w2 = tf.Variable(weights.initialized_value())\n",
    "w3 = tf.Variable(weights.initialized_value() * 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 通过变量实现神经网络的参数并实现前向传播的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.7663565   1.1285427   0.57783246]]\n",
      "[[3.957578]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 声明 w1, w2 两个变量. 这里还通过 seed 参数设定了随机种子.\n",
    "# 这样可以保证每次运行得到的结果是一样的.\n",
    "w1 = tf.Variable(tf.random_normal(shape=(2, 3), stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal(shape=(3, 1), stddev=1, seed=1))\n",
    "\n",
    "# 暂时将输入的特征向量定义为一个常量, 注意这里 x 是一个 1*2 的矩阵\n",
    "x = tf.constant([[0.7, 0.9]])\n",
    "\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "# 使用该函数可以初始化所有变量, 它也会自动处理变量之间的依赖关系\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 通过 TensorFlow 训练神经网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Courier New\">\n",
    "    \n",
    "**监督学习最重要的思想就是:** 在已知答案的标注数据集上, 模型给出的预测结果要尽量接近真实的答案. 通过调整神经网络中的参数对训练数据进行拟合, 可以使得模型对未知的样本提供预测的能力.<br><br>\n",
    "在神经网络优化算法中, 最常用的方法就是**反向传播算法!**<br><br>\n",
    "反向传播算法实现了一个迭代的过程, 在每次迭代的开始, 首先需要取一小部分训练数据, 这一小部分数据叫做 batch. 在上述示例中尝试使用常量来表达一个样例.<br>\n",
    "`x = tf.constant([[0.7, 0.9]])`<br>\n",
    "但如果每轮迭代中选取的数据都用常量表示, 那么 TensorFlow 的计算图将会太大. 为了避免这个问题, TensorFlow 提供了 placeholder 机制用于提供输入数据. **placeholder 相当于定义了一个位置, 这个位置中的数据在程序运行时再指定.** 这样程序在运行中就不要要大量的常量来提供数据."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 下面通过 placeholder 实现前向传播算法(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.957578]]\n"
     ]
    }
   ],
   "source": [
    "w1 = tf.Variable(tf.random_normal(shape=(2, 3), stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal(shape=(3, 1), stddev=1, seed=1))\n",
    "\n",
    "# 定义 placeholder 作为存放输入数据的地方, 这里维度也不一定要定义.\n",
    "# 但如果维度是确定的, 那么给出的维度可以降低出错的概率\n",
    "x = tf.placeholder(shape=(1, 2), dtype=tf.float32, name=\"input\")\n",
    "\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(y, feed_dict={x: [[0.7, 0.9]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Courier New\">\n",
    "    \n",
    "如果将输入的 1x2 矩阵改为 nx2 矩阵, 那么就可以得到 n 个样例的前向传播结果了, 其中 nx2 矩阵的每一行为一个样例数据.<br>\n",
    "这样前向传播的结果为 nx1 的矩阵, 这个矩阵的每一样就代表了一个样例的前向传播结果.\n",
    "### 1.3.2 下面通过 placeholder 实现前向传播算法 (2)\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "[[3.957578 ]\n",
      " [1.1537654]\n",
      " [3.1674924]]\n"
     ]
    }
   ],
   "source": [
    "w1 = tf.Variable(tf.random_normal(shape=(2, 3), stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal(shape=(3, 1), stddev=1, seed=1))\n",
    "\n",
    "# 定义 placeholder 作为存放输入数据的地方, 这里维度也不一定要定义.\n",
    "# 但如果维度是确定的, 那么给出的维度可以降低出错的概率\n",
    "x = tf.placeholder(shape=(3, 2), dtype=tf.float32, name=\"input\")\n",
    "\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    result = sess.run(y, feed_dict={x: [[0.7, 0.9], [0.1, 0.4], [0.5, 0.8]]})\n",
    "    print(result.shape)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Courier New\">\n",
    "    \n",
    "&ensp;&ensp;&ensp;&ensp;在得到一个 batch 的前向传播结果之后, **需要定义一个损失函数来刻画当前的预测值和真实值答案之间的差距**, 然后通过反向传播算法来调整神经网络的取值, 使得差距可以被缩小."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 以下通过定义一个简单的损失函数, 并通过 TensorFlow 定义了反向传播的算法."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数来刻画预测值与真实值的差距\n",
    "corss_entropy = tf.reduce_mean(y * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n",
    "# 定义学习率\n",
    "learning_rate = 0.001\n",
    "# 定义反向传播算法来优化神经网络中的参数\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(corss_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Courier New\">\n",
    "\n",
    "`corss_entropy` 定义了真实值与预测值之间的交叉熵, 这是分类问题中一个常用的损失函数.<br>\n",
    "`train_step` 定义了反向传播的优化方法, 目前 `TensorFlow` 支持7种不同的优化器, 读者可以根据具体的应用选择不同的优化算法. <br>比较常用的优化方法有三种: `tf.train.GradientDescentOptimizer, tf.train.AdamOptimizer` 和 `tf.train.MomentumOptimizer`. <br>\n",
    "在定义了反向传播算法之后, 通过运行 `sess.run(train_step)` 就可以对用在 `GraphKeys.TRAINABLE_VARIABLES` 集合中的变量进行优化, 使得在当前下 `batch` 下损失函数更小."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 完整的神经网络样例程序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面给出了一个完整的程序来训练神经网络结果二分类的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在训练之前神经网络参数的值:\n",
      "w1: [[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "w2: [[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "After 0 training step(s), cross entropy on all data is 0.0674925\n",
      "After 1000 training step(s), cross entropy on all data is 0.0163385\n",
      "After 2000 training step(s), cross entropy on all data is 0.00907547\n",
      "After 3000 training step(s), cross entropy on all data is 0.00714436\n",
      "After 4000 training step(s), cross entropy on all data is 0.00578471\n",
      "[[-1.9618274  2.582354   1.6820377]\n",
      " [-3.4681718  1.0698233  2.11789  ]]\n",
      "[[-1.8247149]\n",
      " [ 2.6854665]\n",
      " [ 1.418195 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 定义训练数据 batch 的大小\n",
    "batch_size = 8\n",
    "\n",
    "# 定义神经网络的参数\n",
    "w1 = tf.Variable(tf.random_normal(shape=(2, 3), stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal(shape=(3, 1), stddev=1, seed=1))\n",
    "\n",
    "# 在 shape 的一个维度上使用 None 可以方便使用不大的 batch 大小, 在训练时需要把数据分成比较小的 batch, \n",
    "# 但是在测试时, 可以一次性使用全部的数据. 当数据集比较小是这样比较方便测试, 但数据集比较大时, \n",
    "# 将大量数据放入一个 batch 可能会导致内存溢出.\n",
    "x = tf.placeholder(dtype=tf.float32, shape=(None, 2), name=\"x-input\")\n",
    "y_ = tf.placeholder(dtype=tf.float32, shape=(None, 1), name=\"y-input\")\n",
    "\n",
    "# 定义神经网络前向传播的过程\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "# 定义损失函数和反向传播算法\n",
    "cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "\n",
    "# 通过随机生成一个模拟数据集\n",
    "rdm = np.random.RandomState(1)\n",
    "dataset_size = 128\n",
    "X = rdm.rand(dataset_size, 2)\n",
    "\n",
    "# 定义规则来给出样本的标签, 在这里所有 x1+x2<1 的样例都被认为是正样本(比如零件合格), 而其他为负样本(比如零件不合格)\n",
    "# 在这里使用 0 来表示负样本, 1 来表示正样本.\n",
    "Y = [[int(x1+x2 < 1)] for (x1, x2) in X]\n",
    "\n",
    "# 创建一个回话来运行 TensorFlow 程序\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # 初始化所有变量\n",
    "    print(\"在训练之前神经网络参数的值:\")\n",
    "    print(\"w1:\", sess.run(w1))\n",
    "    print(\"w2:\", sess.run(w2))\n",
    "\n",
    "    # 设定训练的轮数\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        # 每次选取 batch_size 个样本进行训练\n",
    "        start = (i * batch_size) % dataset_size\n",
    "        end = min(start + batch_size, dataset_size)\n",
    "\n",
    "        # 通过选取的样本训练神经网络并更新参数\n",
    "        sess.run(train_step, feed_dict={x: X[start: end], y_: Y[start: end]})\n",
    "        if i % 1000 == 0:# 每隔一段时间计算在所有数据上的交叉熵并输出\n",
    "            total_cross_entropy = sess.run(cross_entropy, feed_dict={x: X, y_: Y})\n",
    "            \n",
    "            #通过这个结果可以发现随着训练的进行, 交叉熵是逐渐变小的, 交叉熵越小, 说明预测的结果和真实的结果差距越小\n",
    "            print(\"After %d training step(s), cross entropy on all data is %g\" %(i, total_cross_entropy))\n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Courier New\">\n",
    "\n",
    "**上面的程序实现了神经网络的全部过程, 从这段程序可以总结出训练神经网络的过程可以分为一下三个步骤:**\n",
    "1. 定义神经网络的结果和前向传播的输出结果.\n",
    "2. 定义损失函数以及选择反向传播优化的算法.\n",
    "3. 生成回话 `tf.Session()` 并且在训练数据上反复运行反向传播优化算法."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二 深层神经网络\n",
    "## 2.1 深层学习与深层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Courier New\">\n",
    "    \n",
    "深度学习有两个非常重要的特性 -- **多层和非线性**<br>\n",
    "在线性模型中, 模型的输出为输入的加权和. 假设一个模型的输出 y 和输入 $x_{i}$ 满足一下关系, 那么这个模型就是一个线性模型:\n",
    "$$y=\\sum_{i}(w_{i}x_{i} + b)$$\n",
    "其中 $w_{i}, b \\in R$ 为模型的参数, 被称之为线性模型是因为当模型的输入只有一个的时候, x 和 y 形成了二维坐标系上的一条直线. 类似的, 方模型有 n 个输入时, x 和 y 形成了 n+1 维空间中的一个平面. **而一个线性模型中通过输入得到的函数被称之为一个线性变换. 上述公式就是一个线性变换**. 线性模型的最大特点就是任意线性模型的组合仍然还是线性模型.<br>\n",
    "前向传播的计算公式为:<br>\n",
    "$$a^{(1)} = xW^{(1)}, y = a^{(1)}W^{(2)}$$\n",
    "其中 x 为输入, W 为参数, 整理一下上面的公式可以得到模型的输出为:\n",
    "$$y = (xW^{(1)})W^{(2)}$$\n",
    "根据矩阵乘法的结合律有:\n",
    "$$y = x(W^{(1)}W^{(2)}) = xW^{'}$$\n",
    "而 $W^{(1)}W^{(2)}$ 其实可以被表示为一个新的参数$W^{'}$:\n",
    "$$W^{'}=W^{(1)}W^{(2)}=\n",
    "\\left[\\begin{matrix} \n",
    "   W_{1,1}^{(1)} & W_{1,2}^{(1)} & W_{1,3}^{(1)} \\\\\n",
    "   W_{2,1}^{(1)} & W_{2,2}^{(1)} & W_{2,3}^{(1)} \n",
    "\\end{matrix}\\right] \n",
    "\\left[\\begin{matrix} \n",
    "   W_{1,1}^{(2)} \\\\\n",
    "   W_{2,1}^{(2)} \\\\\n",
    "   W_{3,1}^{(2)}\n",
    "\\end{matrix}\\right] = \n",
    "\\left[\\begin{matrix} \n",
    "   W_{1,1}^{(1)}W_{1,1}^{(2)} + W_{1,2}^{(1)}W_{2,1}^{(2)} + W_{1,3}^{(1)}W_{3,1}^{(2)} \\\\\n",
    "   W_{2,1}^{(1)}W_{1,1}^{(2)} + W_{2,2}^{(1)}W_{2,1}^{(2)} + W_{2,3}^{(1)}W_{3,1}^{(2)}\n",
    "\\end{matrix}\\right] = \n",
    "\\left[\\begin{matrix} \n",
    "   W_{1}^{'} \\\\\n",
    "   W_{2}^{'} \n",
    "\\end{matrix}\\right]$$\n",
    "这样输入和输出的关系就可以表示为:\n",
    "$$y = xW^{'} = \n",
    "\\left[\\begin{matrix} \n",
    "   x_{1} & x_{2}\n",
    "\\end{matrix}\\right]\n",
    "\\left[\\begin{matrix} \n",
    "   W_{1}^{'} \\\\\n",
    "   W_{2}^{'} \n",
    "\\end{matrix}\\right] =\n",
    "\\left[\\begin{matrix} \n",
    "   W_{1}^{'}x_{1} + W_{2}^{'}x_{2}\n",
    "\\end{matrix}\\right]$$\n",
    "其中 $W^{'}$ 是新的参数. 这个前向传播的算法完全符合线性模型的定义, 从这个例子可以看到, 虽然这个神经网络有两层(不算输入层), 但是它和单层的神经网络并没有区别. 依此了推, 只通过线性变换, 任意层的全连接神经网络和单层神经网络模型的表达能力没有任何区别, 而且他们都是线性变换. **然而线性模型能够解决问题是有限的, 这就是线性模型最大的局限性.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 常用的神经网络激活函数的函数图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def ReLU(fig):\n",
    "    # ReLU 函数\n",
    "    ax = fig.add_subplot(1, 3, 1)\n",
    "    x = np.arange(-10, 10)\n",
    "    y = np.where(x<0, 0, x)\n",
    "\n",
    "    plt.xlim(-11,11)\n",
    "    plt.ylim(0,11)\n",
    "\n",
    "    # 去除顶部与右侧的边框\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.spines['right'].set_color('none')\n",
    "\n",
    "    # 将下面的边框移动到 y=0 的位置\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.spines['bottom'].set_position(('data',0))\n",
    "    ax.set_xticks([-10,-5,0,5,10])\n",
    "\n",
    "    # 将左侧边框移动到 x=0 的位置\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.spines['left'].set_position(('data',0))\n",
    "    ax.set_yticks([5,10])\n",
    "\n",
    "    ax.plot(x, y, label=\"ReLU\")\n",
    "    ax.legend()\n",
    "    \n",
    "def sigmoid(fig):\n",
    "    # sigmoid 函数\n",
    "    ax = fig.add_subplot(1, 3, 2)\n",
    "    x = np.arange(-10, 10, 0.1)\n",
    "    y = 1/(1+np.exp(-x))\n",
    "\n",
    "    plt.xlim(-11,11)\n",
    "    plt.ylim(0,1.1)\n",
    "\n",
    "    # 去除顶部与右侧的边框\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.spines['right'].set_color('none')\n",
    "\n",
    "    # 将下面的边框移动到 y=0 的位置\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.spines['bottom'].set_position(('data',0))\n",
    "    ax.set_xticks([-10,-5,0,5,10])\n",
    "\n",
    "    # 将左侧边框移动到 x=0 的位置\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.spines['left'].set_position(('data',0))\n",
    "    ax.set_yticks([0,1.0])\n",
    "\n",
    "    ax.plot(x, y, label=\"sigmoid\")\n",
    "    ax.legend()\n",
    "    \n",
    "def tanh(fig):\n",
    "    # tanh 函数\n",
    "    ax = fig.add_subplot(1, 3, 3)\n",
    "    x = np.arange(-10, 10, 0.1)\n",
    "    y = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "    plt.xlim(-11,11)\n",
    "    plt.ylim(-1.2,1.2)\n",
    "\n",
    "    # 去除顶部与右侧的边框\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.spines['right'].set_color('none')\n",
    "\n",
    "    # 将下面的边框移动到 y=0 的位置\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.spines['bottom'].set_position(('data',0))\n",
    "    ax.set_xticks([-10,-5,0,5,10])\n",
    "\n",
    "    # 将左侧边框移动到 x=0 的位置\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.spines['left'].set_position(('data',0))\n",
    "    ax.set_yticks([-1.0,1.0])\n",
    "\n",
    "    ax.plot(x, y, label=\"tanh\")\n",
    "    ax.legend()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fig = plt.figure(figsize=(10, 2))\n",
    "    ReLU(fig)\n",
    "    sigmoid(fig)\n",
    "    tanh(fig)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Courier New\">\n",
    "\n",
    "目前 TensorFlow 提供了7中不同的非线性激活函数, `tf.nn.relu`, `tf.sigmoid` 和 `tf.tanh` 是比较常用的几个. TensorFlow 也支持使用自己定义的激活函数.<br>\n",
    "下面代码展示了如何通过 TensorFlow 实现\"加入偏置项和激活函数的神经网络结构图\"中神经网络的前向传播算法.<br>\n",
    "`a = tf.nn.relu(tf.matmul(x, w1) + biases1)`<br>\n",
    "`y = tf.nn.relu(tf.matmul(a, w2) + biases2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 经典损失函数\n",
    "\n",
    "**分类问题和回归问题是监督学习的两大种类**. 这一小节介绍分类问题和回归问题中使用到的经典损失函数.<br>\n",
    "分类问题希望解决的是将不同的样本分到事先定义好的类别中.<br>                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
